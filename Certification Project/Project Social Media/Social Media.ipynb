{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                               0\n",
       "timedelta                         0\n",
       "n_tokens_title                    0\n",
       "n_tokens_content                  0\n",
       "n_unique_tokens                   0\n",
       "n_non_stop_words                  0\n",
       "n_non_stop_unique_tokens          0\n",
       "num_hrefs                         0\n",
       "num_self_hrefs                    0\n",
       "num_imgs                          0\n",
       "num_videos                        0\n",
       "average_token_length              0\n",
       "num_keywords                      0\n",
       "data_channel_is_lifestyle         0\n",
       " data_channel_is_entertainment    0\n",
       "data_channel_is_bus               0\n",
       "data_channel_is_socmed            0\n",
       "data_channel_is_tech              0\n",
       "data_channel_is_world             0\n",
       "kw_min_min                        0\n",
       "kw_max_min                        0\n",
       "kw_avg_min                        0\n",
       "kw_min_max                        0\n",
       "kw_max_max                        0\n",
       "kw_avg_max                        0\n",
       "kw_min_avg                        0\n",
       "kw_max_avg                        0\n",
       "kw_avg_avg                        0\n",
       "self_reference_min_shares         0\n",
       "self_reference_max_shares         0\n",
       "                                 ..\n",
       "weekday_is_monday                 0\n",
       "weekday_is_tuesday                0\n",
       "weekday_is_wednesday              0\n",
       "weekday_is_thursday               0\n",
       "weekday_is_friday                 0\n",
       "weekday_is_saturday               0\n",
       "weekday_is_sunday                 0\n",
       "is_weekend                        0\n",
       "LDA_00                            0\n",
       "LDA_01                            0\n",
       "LDA_02                            0\n",
       "LDA_03                            0\n",
       "LDA_04                            0\n",
       "global_subjectivity               0\n",
       "global_sentiment_polarity         0\n",
       "global_rate_positive_words        0\n",
       "global_rate_negative_words        0\n",
       "rate_positive_words               0\n",
       "rate_negative_words               0\n",
       "avg_positive_polarity             0\n",
       "min_positive_polarity             0\n",
       "max_positive_polarity             0\n",
       "avg_negative_polarity             0\n",
       "min_negative_polarity             0\n",
       "max_negative_polarity             0\n",
       "title_subjectivity                0\n",
       "title_sentiment_polarity          0\n",
       "abs_title_subjectivity            0\n",
       "abs_title_sentiment_polarity      0\n",
       "shares                            0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for Nulls\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of shares\n",
    "y = dataset.iloc[:,60].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,1:60].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.022</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.021</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   12.68</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 17 Jul 2019</td> <th>  Prob (F-statistic):</th>  <td>1.39e-114</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:14:07</td>     <th>  Log-Likelihood:    </th> <td>-3.4228e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 31715</td>      <th>  AIC:               </th>  <td>6.847e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 31658</td>      <th>  BIC:               </th>  <td>6.852e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    57</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>  346.0862</td> <td>   96.077</td> <td>    3.602</td> <td> 0.000</td> <td>  157.771</td> <td>  534.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>  236.2730</td> <td>   70.521</td> <td>    3.350</td> <td> 0.001</td> <td>   98.048</td> <td>  374.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>  387.8900</td> <td>  119.467</td> <td>    3.247</td> <td> 0.001</td> <td>  153.730</td> <td>  622.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td> 1.891e+04</td> <td> 7731.005</td> <td>    2.446</td> <td> 0.014</td> <td> 3759.694</td> <td> 3.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>-7105.7121</td> <td> 3.19e+04</td> <td>   -0.223</td> <td> 0.824</td> <td>-6.96e+04</td> <td> 5.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>-9793.2214</td> <td> 6081.628</td> <td>   -1.610</td> <td> 0.107</td> <td>-2.17e+04</td> <td> 2127.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>  213.0870</td> <td>   86.700</td> <td>    2.458</td> <td> 0.014</td> <td>   43.151</td> <td>  383.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td> -262.5661</td> <td>   79.229</td> <td>   -3.314</td> <td> 0.001</td> <td> -417.858</td> <td> -107.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>   49.5978</td> <td>   84.932</td> <td>    0.584</td> <td> 0.559</td> <td> -116.873</td> <td>  216.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>   56.5373</td> <td>   73.687</td> <td>    0.767</td> <td> 0.443</td> <td>  -87.893</td> <td>  200.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th> <td> -441.9228</td> <td>  234.872</td> <td>   -1.882</td> <td> 0.060</td> <td> -902.281</td> <td>   18.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th> <td>  166.3950</td> <td>   81.231</td> <td>    2.048</td> <td> 0.041</td> <td>    7.179</td> <td>  325.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th> <td> -155.4173</td> <td>  100.906</td> <td>   -1.540</td> <td> 0.124</td> <td> -353.198</td> <td>   42.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th> <td> -378.7877</td> <td>  111.917</td> <td>   -3.385</td> <td> 0.001</td> <td> -598.150</td> <td> -159.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th> <td> -183.3941</td> <td>  159.077</td> <td>   -1.153</td> <td> 0.249</td> <td> -495.191</td> <td>  128.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th> <td>  -55.3619</td> <td>   99.799</td> <td>   -0.555</td> <td> 0.579</td> <td> -250.972</td> <td>  140.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th> <td>  -66.4511</td> <td>  164.938</td> <td>   -0.403</td> <td> 0.687</td> <td> -389.737</td> <td>  256.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th> <td>  -23.0593</td> <td>  176.137</td> <td>   -0.131</td> <td> 0.896</td> <td> -368.294</td> <td>  322.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th> <td>  137.9961</td> <td>  130.243</td> <td>    1.060</td> <td> 0.289</td> <td> -117.285</td> <td>  393.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th> <td>  246.5277</td> <td>  209.631</td> <td>    1.176</td> <td> 0.240</td> <td> -164.357</td> <td>  657.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th> <td> -142.5159</td> <td>  206.580</td> <td>   -0.690</td> <td> 0.490</td> <td> -547.420</td> <td>  262.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th> <td> -118.1467</td> <td>   78.589</td> <td>   -1.503</td> <td> 0.133</td> <td> -272.184</td> <td>   35.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th> <td>   -0.3272</td> <td>  145.070</td> <td>   -0.002</td> <td> 0.998</td> <td> -284.670</td> <td>  284.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th> <td>  -16.9939</td> <td>  131.056</td> <td>   -0.130</td> <td> 0.897</td> <td> -273.869</td> <td>  239.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th> <td> -341.7176</td> <td>   98.421</td> <td>   -3.472</td> <td> 0.001</td> <td> -534.627</td> <td> -148.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th> <td>-1205.4656</td> <td>  175.497</td> <td>   -6.869</td> <td> 0.000</td> <td>-1549.447</td> <td> -861.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th> <td> 2149.2045</td> <td>  215.679</td> <td>    9.965</td> <td> 0.000</td> <td> 1726.466</td> <td> 2571.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th> <td>  529.9049</td> <td>  163.393</td> <td>    3.243</td> <td> 0.001</td> <td>  209.648</td> <td>  850.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th> <td>  255.0362</td> <td>  198.043</td> <td>    1.288</td> <td> 0.198</td> <td> -133.136</td> <td>  643.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th> <td> -184.6444</td> <td>  285.717</td> <td>   -0.646</td> <td> 0.518</td> <td> -744.661</td> <td>  375.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th> <td>  160.4683</td> <td>   54.410</td> <td>    2.949</td> <td> 0.003</td> <td>   53.823</td> <td>  267.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th> <td>  -46.0750</td> <td>   53.350</td> <td>   -0.864</td> <td> 0.388</td> <td> -150.643</td> <td>   58.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th> <td>  -42.4799</td> <td>   53.494</td> <td>   -0.794</td> <td> 0.427</td> <td> -147.330</td> <td>   62.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th> <td>  -70.9449</td> <td>   53.637</td> <td>   -1.323</td> <td> 0.186</td> <td> -176.076</td> <td>   34.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th> <td>  -48.6955</td> <td>   55.865</td> <td>   -0.872</td> <td> 0.383</td> <td> -158.192</td> <td>   60.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th> <td>  110.1377</td> <td>   51.098</td> <td>    2.155</td> <td> 0.031</td> <td>    9.984</td> <td>  210.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th> <td>  -29.4656</td> <td>   49.762</td> <td>   -0.592</td> <td> 0.554</td> <td> -127.002</td> <td>   68.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th> <td>   56.5219</td> <td>   32.127</td> <td>    1.759</td> <td> 0.079</td> <td>   -6.449</td> <td>  119.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th> <td> 1.038e+05</td> <td> 1.66e+06</td> <td>    0.063</td> <td> 0.950</td> <td>-3.15e+06</td> <td> 3.36e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th> <td>  8.66e+04</td> <td> 1.39e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-2.63e+06</td> <td> 2.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th> <td>  1.11e+05</td> <td> 1.78e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.38e+06</td> <td>  3.6e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th> <td> 1.164e+05</td> <td> 1.86e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.54e+06</td> <td> 3.77e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th> <td>  1.14e+05</td> <td> 1.83e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.47e+06</td> <td> 3.69e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th> <td>  254.7305</td> <td>  113.831</td> <td>    2.238</td> <td> 0.025</td> <td>   31.617</td> <td>  477.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th> <td>   47.5248</td> <td>  185.523</td> <td>    0.256</td> <td> 0.798</td> <td> -316.108</td> <td>  411.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th> <td> -343.4411</td> <td>  144.518</td> <td>   -2.376</td> <td> 0.017</td> <td> -626.703</td> <td>  -60.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th> <td>   -0.5963</td> <td>  172.370</td> <td>   -0.003</td> <td> 0.997</td> <td> -338.447</td> <td>  337.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th> <td>  303.4569</td> <td> 1127.012</td> <td>    0.269</td> <td> 0.788</td> <td>-1905.530</td> <td> 2512.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th> <td>  188.7744</td> <td>  934.501</td> <td>    0.202</td> <td> 0.840</td> <td>-1642.884</td> <td> 2020.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th> <td>  -59.4664</td> <td>  163.756</td> <td>   -0.363</td> <td> 0.717</td> <td> -380.434</td> <td>  261.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th> <td> -179.5387</td> <td>   93.724</td> <td>   -1.916</td> <td> 0.055</td> <td> -363.241</td> <td>    4.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th> <td>   64.9207</td> <td>  122.148</td> <td>    0.531</td> <td> 0.595</td> <td> -174.494</td> <td>  304.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th> <td> -269.0167</td> <td>  184.346</td> <td>   -1.459</td> <td> 0.144</td> <td> -630.343</td> <td>   92.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th> <td>   -0.9319</td> <td>  152.750</td> <td>   -0.006</td> <td> 0.995</td> <td> -300.327</td> <td>  298.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th> <td>   29.4675</td> <td>  114.019</td> <td>    0.258</td> <td> 0.796</td> <td> -194.014</td> <td>  252.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th> <td>   60.8415</td> <td>  101.432</td> <td>    0.600</td> <td> 0.549</td> <td> -137.970</td> <td>  259.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th> <td>  101.8147</td> <td>   75.955</td> <td>    1.340</td> <td> 0.180</td> <td>  -47.060</td> <td>  250.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th> <td>  144.9782</td> <td>   78.792</td> <td>    1.840</td> <td> 0.066</td> <td>   -9.456</td> <td>  299.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th> <td>   33.2960</td> <td>  102.193</td> <td>    0.326</td> <td> 0.745</td> <td> -167.007</td> <td>  233.599</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>84024.349</td> <th>  Durbin-Watson:     </th>    <td>   1.830</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>3206881176.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>31.536</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>1559.533</td>  <th>  Cond. No.          </th>    <td>5.07e+15</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.93e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.022\n",
       "Model:                            OLS   Adj. R-squared:                  0.021\n",
       "Method:                 Least Squares   F-statistic:                     12.68\n",
       "Date:                Wed, 17 Jul 2019   Prob (F-statistic):          1.39e-114\n",
       "Time:                        13:14:07   Log-Likelihood:            -3.4228e+05\n",
       "No. Observations:               31715   AIC:                         6.847e+05\n",
       "Df Residuals:                   31658   BIC:                         6.852e+05\n",
       "Df Model:                          57                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1           346.0862     96.077      3.602      0.000     157.771     534.401\n",
       "x2           236.2730     70.521      3.350      0.001      98.048     374.497\n",
       "x3           387.8900    119.467      3.247      0.001     153.730     622.050\n",
       "x4          1.891e+04   7731.005      2.446      0.014    3759.694    3.41e+04\n",
       "x5         -7105.7121   3.19e+04     -0.223      0.824   -6.96e+04    5.54e+04\n",
       "x6         -9793.2214   6081.628     -1.610      0.107   -2.17e+04    2127.007\n",
       "x7           213.0870     86.700      2.458      0.014      43.151     383.023\n",
       "x8          -262.5661     79.229     -3.314      0.001    -417.858    -107.274\n",
       "x9            49.5978     84.932      0.584      0.559    -116.873     216.068\n",
       "x10           56.5373     73.687      0.767      0.443     -87.893     200.968\n",
       "x11         -441.9228    234.872     -1.882      0.060    -902.281      18.436\n",
       "x12          166.3950     81.231      2.048      0.041       7.179     325.611\n",
       "x13         -155.4173    100.906     -1.540      0.124    -353.198      42.363\n",
       "x14         -378.7877    111.917     -3.385      0.001    -598.150    -159.425\n",
       "x15         -183.3941    159.077     -1.153      0.249    -495.191     128.403\n",
       "x16          -55.3619     99.799     -0.555      0.579    -250.972     140.249\n",
       "x17          -66.4511    164.938     -0.403      0.687    -389.737     256.835\n",
       "x18          -23.0593    176.137     -0.131      0.896    -368.294     322.176\n",
       "x19          137.9961    130.243      1.060      0.289    -117.285     393.277\n",
       "x20          246.5277    209.631      1.176      0.240    -164.357     657.412\n",
       "x21         -142.5159    206.580     -0.690      0.490    -547.420     262.389\n",
       "x22         -118.1467     78.589     -1.503      0.133    -272.184      35.891\n",
       "x23           -0.3272    145.070     -0.002      0.998    -284.670     284.015\n",
       "x24          -16.9939    131.056     -0.130      0.897    -273.869     239.882\n",
       "x25         -341.7176     98.421     -3.472      0.001    -534.627    -148.808\n",
       "x26        -1205.4656    175.497     -6.869      0.000   -1549.447    -861.484\n",
       "x27         2149.2045    215.679      9.965      0.000    1726.466    2571.943\n",
       "x28          529.9049    163.393      3.243      0.001     209.648     850.162\n",
       "x29          255.0362    198.043      1.288      0.198    -133.136     643.208\n",
       "x30         -184.6444    285.717     -0.646      0.518    -744.661     375.372\n",
       "x31          160.4683     54.410      2.949      0.003      53.823     267.114\n",
       "x32          -46.0750     53.350     -0.864      0.388    -150.643      58.493\n",
       "x33          -42.4799     53.494     -0.794      0.427    -147.330      62.370\n",
       "x34          -70.9449     53.637     -1.323      0.186    -176.076      34.186\n",
       "x35          -48.6955     55.865     -0.872      0.383    -158.192      60.801\n",
       "x36          110.1377     51.098      2.155      0.031       9.984     210.291\n",
       "x37          -29.4656     49.762     -0.592      0.554    -127.002      68.071\n",
       "x38           56.5219     32.127      1.759      0.079      -6.449     119.492\n",
       "x39         1.038e+05   1.66e+06      0.063      0.950   -3.15e+06    3.36e+06\n",
       "x40          8.66e+04   1.39e+06      0.062      0.950   -2.63e+06    2.81e+06\n",
       "x41          1.11e+05   1.78e+06      0.062      0.950   -3.38e+06     3.6e+06\n",
       "x42         1.164e+05   1.86e+06      0.062      0.950   -3.54e+06    3.77e+06\n",
       "x43          1.14e+05   1.83e+06      0.062      0.950   -3.47e+06    3.69e+06\n",
       "x44          254.7305    113.831      2.238      0.025      31.617     477.844\n",
       "x45           47.5248    185.523      0.256      0.798    -316.108     411.158\n",
       "x46         -343.4411    144.518     -2.376      0.017    -626.703     -60.180\n",
       "x47           -0.5963    172.370     -0.003      0.997    -338.447     337.255\n",
       "x48          303.4569   1127.012      0.269      0.788   -1905.530    2512.444\n",
       "x49          188.7744    934.501      0.202      0.840   -1642.884    2020.433\n",
       "x50          -59.4664    163.756     -0.363      0.717    -380.434     261.501\n",
       "x51         -179.5387     93.724     -1.916      0.055    -363.241       4.163\n",
       "x52           64.9207    122.148      0.531      0.595    -174.494     304.335\n",
       "x53         -269.0167    184.346     -1.459      0.144    -630.343      92.309\n",
       "x54           -0.9319    152.750     -0.006      0.995    -300.327     298.463\n",
       "x55           29.4675    114.019      0.258      0.796    -194.014     252.949\n",
       "x56           60.8415    101.432      0.600      0.549    -137.970     259.653\n",
       "x57          101.8147     75.955      1.340      0.180     -47.060     250.689\n",
       "x58          144.9782     78.792      1.840      0.066      -9.456     299.413\n",
       "x59           33.2960    102.193      0.326      0.745    -167.007     233.599\n",
       "==============================================================================\n",
       "Omnibus:                    84024.349   Durbin-Watson:                   1.830\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       3206881176.823\n",
       "Skew:                          31.536   Prob(JB):                         0.00\n",
       "Kurtosis:                    1559.533   Cond. No.                     5.07e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.93e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use stat to see and delete x-vars that have |p|>t<0.05\n",
    "import statsmodels.formula.api as sm\n",
    "lm2 = sm.OLS(y_train,x_train).fit()\n",
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new x valuesfrom sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "x = dataset.iloc[:,[1,2,3,7,8,11,14,20,25,26,27,28,29]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Reg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm = lm.fit(x_train,y_train)   #lm.fit(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "y_pred = lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042534203626099276"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best r^2 score I got. You can use lm.predict to predict values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
